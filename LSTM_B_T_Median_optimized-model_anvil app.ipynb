{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM_B-T-Median.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOO2/KPN1BcyViihFI5Czd8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DipankarJDutta/DipankarJDutta-Biomarker-discovery-from-B-T-cells-in-FASD-GDM-with-LSTM/blob/master/LSTM_B_T_Median_optimized-model_anvil%20app.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cg5rCj_kay_U",
        "outputId": "3c54abb5-2483-4417-f33b-bda8f2d7b189",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        }
      },
      "source": [
        "# Installing Anvil (https://anvil.works)\n",
        "!pip install anvil-uplink"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: anvil-uplink in /usr/local/lib/python3.7/dist-packages (0.3.39)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from anvil-uplink) (1.15.0)\n",
            "Requirement already satisfied: ws4py in /usr/local/lib/python3.7/dist-packages (from anvil-uplink) (0.5.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from anvil-uplink) (0.16.0)\n",
            "Collecting argparse\n",
            "  Using cached argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
            "Installing collected packages: argparse\n",
            "Successfully installed argparse-1.4.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "argparse"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bnLcNM4bUzJ"
      },
      "source": [
        "# Connecting Anvil app to this colab notebook\n",
        "import anvil.server\n",
        "\n",
        "anvil.server.connect(\"6TF5SQKF6YQEPBSWDNM7MTOK-7NQB5JD2QCWYHK4U\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfPN8Hkp_22L"
      },
      "source": [
        "# Loading essentials\n",
        "import numpy as np\n",
        "from numpy import loadtxt\n",
        "from numpy import reshape\n",
        "from numpy import array\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "from matplotlib import pyplot"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zN3LIBHE9XH0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "outputId": "4d11791e-f5b9-4bd9-b7a6-1b4f652899ba"
      },
      "source": [
        "# Fixing random seed to 007 for reproducibility\n",
        "from numpy.random import seed\n",
        "seed(7)\n",
        "import tensorflow\n",
        "tensorflow.random"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-5c5c751e4652>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    from tensorflow import set_random_seed(seed)\u001b[0m\n\u001b[0m                                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMq2vKCJNlmL"
      },
      "source": [
        "# Load data\n",
        "dataset = loadtxt('B-T-median.csv', delimiter = ',')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hf6EYfq1OSND"
      },
      "source": [
        "# Reshape 2D dataset into a 3D dataset with columns as features with one time-step\n",
        "x = dataset.reshape(56, 1, 30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbfEdKabnlcK"
      },
      "source": [
        "#Specify binary output of good (1) and bad (0) learners, y, in x. Learner type differentiated by population median.\n",
        "y = x [:, :, -1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBx8zDJlVjje"
      },
      "source": [
        "# Define LSTM (One to One Model) with Sigmoid activation on the output layer for binary classification. Add Dropout to input layer to prevent overfitting between epochs 1-50 without dropout.\n",
        "model = Sequential()\n",
        "model.add(LSTM(3, input_shape=(1,30)))\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1vO6vpKmAgb"
      },
      "source": [
        "#Compile the model \n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x30BNEbImxA5"
      },
      "source": [
        "#Fit the model with a 80-20 split of dataset & shuffling sample order within an epoch\n",
        "history = model.fit(x, y, validation_split = 0.2, batch_size = 8, epochs = 1000, shuffle = True, verbose=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxvuJalq1LHl"
      },
      "source": [
        "#Model Summary\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zu3xqT65BP95"
      },
      "source": [
        "#Diagnostic plot Ia: Model Performance (training loss vs test loss) for 1000 epochs\n",
        "pyplot.plot(history.history['loss'])\n",
        "pyplot.plot(history.history['val_loss'])\n",
        "pyplot.title('Loss in training vs validation datasets')\n",
        "pyplot.ylabel('Loss')\n",
        "pyplot.xlabel('Epoch')\n",
        "pyplot.legend(['Training Loss', 'Validation Loss'], loc='upper right')\n",
        "pyplot.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcTyRp3obafr"
      },
      "source": [
        "#Diagnostic plot Ib: Model Performance (training loss vs test loss) for 750 epochs\n",
        "from matplotlib import pyplot\n",
        "pyplot.plot(history.history['loss'])\n",
        "pyplot.plot(history.history['val_loss'])\n",
        "pyplot.title('Loss in training vs validation datasets')\n",
        "pyplot.xlim(-20, 750)\n",
        "pyplot.ylabel('Loss')\n",
        "pyplot.xlabel('Epoch')\n",
        "pyplot.legend(['Training Loss', 'Validation Loss'], loc='upper right')\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHVc2Bt9diLq"
      },
      "source": [
        "#Diagnostic plot Ic: Model Performance (training loss vs test loss) for 500 epochs\n",
        "from matplotlib import pyplot\n",
        "pyplot.plot(history.history['loss'])\n",
        "pyplot.plot(history.history['val_loss'])\n",
        "pyplot.title('Loss in training vs validation datasets')\n",
        "pyplot.xlim(-20, 500)\n",
        "pyplot.ylabel('Loss')\n",
        "pyplot.xlabel('Epoch')\n",
        "pyplot.legend(['Training Loss', 'Validation Loss'], loc='upper right')\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzLlXzRodzQd"
      },
      "source": [
        "#Diagnostic plot Id: Model Performance (training loss vs test loss) for 250 epochs\n",
        "from matplotlib import pyplot\n",
        "pyplot.plot(history.history['loss'])\n",
        "pyplot.plot(history.history['val_loss'])\n",
        "pyplot.title('Loss in training vs validation datasets')\n",
        "pyplot.xlim(-10, 250)\n",
        "pyplot.ylabel('Loss')\n",
        "pyplot.xlabel('Epoch')\n",
        "pyplot.legend(['Training Loss', 'Validation Loss'], loc='upper right')\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "racnXi9hDEkC"
      },
      "source": [
        "#Diagnostic Plot IIa: Model Performance (training accuracy vs test accuracy) for 1000 epochs\n",
        "pyplot.plot(history.history['acc'])\n",
        "pyplot.plot(history.history['val_acc'])\n",
        "pyplot.title('Accuracy in Training vs Validation datasets')\n",
        "pyplot.ylabel('Accuracy')\n",
        "pyplot.xlabel('Epoch')\n",
        "pyplot.legend(['Training Accuracy', 'Test Accuracy'], loc='lower right')\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aj2xKTTeeKz5"
      },
      "source": [
        "#Diagnostic Plot IIb: Model Performance (training accuracy vs test accuracy) for 750 epochs\n",
        "pyplot.plot(history.history['acc'])\n",
        "pyplot.plot(history.history['val_acc'])\n",
        "pyplot.title('Accuracy in Training vs Validation datasets')\n",
        "pyplot.xlim(-25, 750)\n",
        "pyplot.ylabel('Accuracy')\n",
        "pyplot.xlabel('Epoch')\n",
        "pyplot.legend(['Training Accuracy', 'Test Accuracy'], loc='lower right')\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXcUV5Qhezn1"
      },
      "source": [
        "#Diagnostic Plot IIc: Model Performance (training accuracy vs test accuracy) for 500 epochs\n",
        "pyplot.plot(history.history['acc'])\n",
        "pyplot.plot(history.history['val_acc'])\n",
        "pyplot.title('Accuracy in Training vs Validation datasets')\n",
        "pyplot.xlim(-25, 500)\n",
        "pyplot.ylabel('Accuracy')\n",
        "pyplot.xlabel('Epoch')\n",
        "pyplot.legend(['Training Accuracy', 'Test Accuracy'], loc='lower right')\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfavSt3Ce75i"
      },
      "source": [
        "#Diagnostic Plot IIc: Model Performance (training accuracy vs test accuracy) for 250 epochs\n",
        "pyplot.plot(history.history['acc'])\n",
        "pyplot.plot(history.history['val_acc'])\n",
        "pyplot.title('Accuracy in Training vs Validation datasets')\n",
        "pyplot.xlim(-25, 250)\n",
        "pyplot.ylabel('Accuracy')\n",
        "pyplot.xlabel('Epoch')\n",
        "pyplot.legend(['Training Accuracy', 'Test Accuracy'], loc='lower right')\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}